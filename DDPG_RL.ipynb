{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DDPG_RL.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cxbxmxcx/GoogleColab/blob/master/DDPG_RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYoALdKgSDtK",
        "colab_type": "text"
      },
      "source": [
        "DDPG Converted to Google Colab\n",
        "\n",
        "Original https://github.com/seungeunrho/minimalRL/blob/master/ddpg.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gRfriJ0R8GW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import random\n",
        "import collections\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUhKfYdWSIZD",
        "colab_type": "text"
      },
      "source": [
        "No additional installation needed, all imports are already installed on Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTunoU3nSg2D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Hyperparameters\n",
        "lr_mu        = 0.0005\n",
        "lr_q         = 0.001\n",
        "gamma        = 0.99\n",
        "batch_size   = 32\n",
        "buffer_limit = 50000\n",
        "tau          = 0.005 # for target network soft update\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmmOFYKbSmJo",
        "colab_type": "text"
      },
      "source": [
        "Hyperparameters:\n",
        "lr_mu = \n",
        "lr_q =\n",
        "gamma = discount factor\n",
        "batch_size = the size of experiences replayed as a batch into the DNN\n",
        "buffer_limit =\n",
        "tau ="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bAmJI7OS9LO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer():\n",
        "    def __init__(self):\n",
        "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
        "\n",
        "    def put(self, transition):\n",
        "        self.buffer.append(transition)\n",
        "    \n",
        "    def sample(self, n):\n",
        "        mini_batch = random.sample(self.buffer, n)\n",
        "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
        "\n",
        "        for transition in mini_batch:\n",
        "            s, a, r, s_prime, done_mask = transition\n",
        "            s_lst.append(s)\n",
        "            a_lst.append([a])\n",
        "            r_lst.append([r])\n",
        "            s_prime_lst.append(s_prime)\n",
        "            done_mask_lst.append([done_mask])\n",
        "        \n",
        "        return torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n",
        "               torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
        "               torch.tensor(done_mask_lst)\n",
        "    \n",
        "    def size(self):\n",
        "        return len(self.buffer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAkaTgl-TB17",
        "colab_type": "text"
      },
      "source": [
        "ReplayBuffer as per standard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqjbRH1fTGiA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MuNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MuNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(3, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc_mu = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        mu = torch.tanh(self.fc_mu(x))*2 # Multipled by 2 because the action space of the Pendulum-v0 is [-2,2]\n",
        "        return mu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ws6nSt7TKWe",
        "colab_type": "text"
      },
      "source": [
        "The MuNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHgHUvZqTUEI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class QNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(QNet, self).__init__()\n",
        "        \n",
        "        self.fc_s = nn.Linear(3, 64)\n",
        "        self.fc_a = nn.Linear(1,64)\n",
        "        self.fc_q = nn.Linear(128, 32)\n",
        "        self.fc_3 = nn.Linear(32,1)\n",
        "\n",
        "    def forward(self, x, a):\n",
        "        h1 = F.relu(self.fc_s(x))\n",
        "        h2 = F.relu(self.fc_a(a))\n",
        "        cat = torch.cat([h1,h2], dim=1)\n",
        "        q = F.relu(self.fc_q(cat))\n",
        "        q = self.fc_3(q)\n",
        "        return q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXhRGIKHTX0h",
        "colab_type": "text"
      },
      "source": [
        "QNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lc2cMTR8TZlZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class OrnsteinUhlenbeckNoise:\n",
        "    def __init__(self, mu):\n",
        "        self.theta, self.dt, self.sigma = 0.1, 0.01, 0.1\n",
        "        self.mu = mu\n",
        "        self.x_prev = np.zeros_like(self.mu)\n",
        "\n",
        "    def __call__(self):\n",
        "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
        "                self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
        "        self.x_prev = x\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQpc0j7ITd1Q",
        "colab_type": "text"
      },
      "source": [
        "Noise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WXlCW4xTf5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(mu, mu_target, q, q_target, memory, q_optimizer, mu_optimizer):\n",
        "    s,a,r,s_prime,done_mask  = memory.sample(batch_size)\n",
        "    \n",
        "    target = r + gamma * q_target(s_prime, mu_target(s_prime))\n",
        "    q_loss = F.smooth_l1_loss(q(s,a), target.detach())\n",
        "    q_optimizer.zero_grad()\n",
        "    q_loss.backward()\n",
        "    q_optimizer.step()\n",
        "    \n",
        "    mu_loss = -q(s,mu(s)).mean() # That's all for the policy loss.\n",
        "    mu_optimizer.zero_grad()\n",
        "    mu_loss.backward()\n",
        "    mu_optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5l0GLDoOTmP4",
        "colab_type": "text"
      },
      "source": [
        "training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_1XjNqRTojH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def soft_update(net, net_target):\n",
        "    for param_target, param in zip(net_target.parameters(), net.parameters()):\n",
        "        param_target.data.copy_(param_target.data * (1.0 - tau) + param.data * tau)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZyArDZATr7I",
        "colab_type": "text"
      },
      "source": [
        "soft update"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjgZdhfNTtd2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    env = gym.make('Pendulum-v0')\n",
        "    memory = ReplayBuffer()\n",
        "\n",
        "    q, q_target = QNet(), QNet()\n",
        "    q_target.load_state_dict(q.state_dict())\n",
        "    mu, mu_target = MuNet(), MuNet()\n",
        "    mu_target.load_state_dict(mu.state_dict())\n",
        "\n",
        "    score = 0.0\n",
        "    print_interval = 20\n",
        "\n",
        "    mu_optimizer = optim.Adam(mu.parameters(), lr=lr_mu)\n",
        "    q_optimizer  = optim.Adam(q.parameters(), lr=lr_q)\n",
        "    ou_noise = OrnsteinUhlenbeckNoise(mu=np.zeros(1))\n",
        "\n",
        "    for n_epi in range(10000):\n",
        "        s = env.reset()\n",
        "        \n",
        "        for t in range(300): # maximum length of episode is 200 for Pendulum-v0\n",
        "            a = mu(torch.from_numpy(s).float()) \n",
        "            a = a.item() + ou_noise()[0]\n",
        "            s_prime, r, done, info = env.step([a])\n",
        "            memory.put((s,a,r/100.0,s_prime,done))\n",
        "            score +=r\n",
        "            s = s_prime\n",
        "\n",
        "            if done:\n",
        "                break              \n",
        "                \n",
        "        if memory.size()>2000:\n",
        "            for i in range(10):\n",
        "                train(mu, mu_target, q, q_target, memory, q_optimizer, mu_optimizer)\n",
        "                soft_update(mu, mu_target)\n",
        "                soft_update(q,  q_target)\n",
        "        \n",
        "        if n_epi%print_interval==0 and n_epi!=0:\n",
        "            print(\"# of episode :{}, avg score : {:.1f}\".format(n_epi, score/print_interval))\n",
        "            score = 0.0\n",
        "\n",
        "    env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXXgVDFjT2pi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "5ea99dd0-f432-47d0-9428-ea15a7e11134"
      },
      "source": [
        "main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of episode :20, avg score : -1590.4\n",
            "# of episode :40, avg score : -1568.7\n",
            "# of episode :60, avg score : -1737.6\n",
            "# of episode :80, avg score : -1607.2\n",
            "# of episode :100, avg score : -1531.0\n",
            "# of episode :120, avg score : -1477.8\n",
            "# of episode :140, avg score : -1330.5\n",
            "# of episode :160, avg score : -1269.2\n",
            "# of episode :180, avg score : -1171.7\n",
            "# of episode :200, avg score : -1074.9\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}